{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TASK\n",
    "## Deadline: 31 martie ora 23:59.\n",
    "\n",
    "Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n",
    "\n",
    "Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n",
    "1. Impartiti setul de date in 80% train, 10% validare si 10% test\n",
    "2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n",
    "3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n",
    "4. Implementati urmatoarea arhitectura:\n",
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n",
    "5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n",
    "6. Evaluati cel mai bun model obtinut pe datele de test.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7f6973ed9190>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from sys import platform, path\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    path.append('/home/dariusbuhai/python/lib/python3.9/site-packages')\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "train_df, test_df = train_test_split(data, test_size=0.1, random_state=1)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/dariusbuhai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def transform_to_tokens(data):\n",
    "    reviews = []\n",
    "    for review in data:\n",
    "        review_tokenized = word_tokenize(review.lower())\n",
    "        reviews.append(review_tokenized)\n",
    "    return reviews\n",
    "\n",
    "train_reviews = transform_to_tokens(train_df.review)\n",
    "test_reviews = transform_to_tokens(test_df.review)\n",
    "val_reviews = transform_to_tokens(val_df.review)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def no_punctuation_or_stopwords(data):\n",
    "    all_words = []\n",
    "    for word in data:\n",
    "        if word not in nlp.Defaults.stop_words and word != ' ' and word not in string.punctuation:\n",
    "            all_words.append(word)\n",
    "    return all_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "def word_freq(data, min_aparitions):\n",
    "\n",
    "    all_words = [words.lower() for sentences in data for words in sentences]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n",
    "    final_vocab = [k for k,v in sorted_vocab if v > min_aparitions]\n",
    "\n",
    "    return final_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def create_vocab(reviews):\n",
    "    vocab = word_freq(reviews, min_aparitions = 18)\n",
    "    vocab = no_punctuation_or_stopwords(vocab)\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "vocab = create_vocab(train_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18409\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sentences(data, char_indices, one_hot = False):\n",
    "    vectorized = []\n",
    "    for sentences in data:\n",
    "\n",
    "        # transformam fiecare review in reprezentarea lui sub forma de indici ale caracterelor continute\n",
    "        sentences_of_indices = [char_indices[w] if w in char_indices.keys() else char_indices['UNK'] for w in sentences]\n",
    "\n",
    "        # pentru fiecare indice putem face reprezentarea one-hot corespunzatoare\n",
    "        # sau putem sa nu facem asta si sa adaugam un embedding layer in model care face această transformare\n",
    "        if one_hot:\n",
    "            sentences_of_indices = np.eye(len(char_indices))[sentences_of_indices]\n",
    "\n",
    "        vectorized.append(sentences_of_indices)\n",
    "\n",
    "    return vectorized\n",
    "\n",
    "def pad(samples, max_length):\n",
    "\n",
    "    return torch.tensor([\n",
    "        sample[:max_length] + [1] * max(0, max_length - len(sample))\n",
    "        for sample in samples\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def create_vectorize(vocab, reviews):\n",
    "    word_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n",
    "    indices_word = dict((i + 2, c) for i, c in enumerate(vocab))\n",
    "    indices_word[0] = 'UNK'\n",
    "    word_indices['UNK'] = 0\n",
    "    indices_word[1] = 'PAD'\n",
    "    word_indices['PAD'] = 1\n",
    "\n",
    "    reviews_vectorized = vectorize_sentences(reviews, word_indices)\n",
    "    reviews_vectorized = pad(reviews_vectorized, max_length = 800)\n",
    "    return reviews_vectorized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "train_reviews_vectorized = create_vectorize(vocab, train_reviews)\n",
    "test_reviews_vectorized = create_vectorize(vocab, test_reviews)\n",
    "val_reviews_vectorized = create_vectorize(vocab, val_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40500, 800])\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews_vectorized.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n",
    "        return self.samples[k], self.labels[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n",
    "        return len(self.samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(train_reviews_vectorized.shape[0], 100, padding_idx=1)\n",
    "        conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        global_average = torch.nn.AvgPool1d(kernel_size=100, stride=128)\n",
    "        self.convolutions = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.4),\n",
    "            conv1,\n",
    "            conv2,\n",
    "            conv3,\n",
    "            global_average\n",
    "        )\n",
    "\n",
    "        # Flattening layer\n",
    "        flatten = torch.nn.Flatten()\n",
    "\n",
    "        # Linear layer cu 128 input features și 2 outputs fără funcție de activare\n",
    "        linear = torch.nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(flatten, linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        output = self.convolutions(embeddings)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Adam optimizer cu lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Cross Entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_ds = Dataset(train_reviews_vectorized, train_df['sentiment'].tolist())\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "val_ds = Dataset(val_reviews_vectorized, val_df['sentiment'].tolist())\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "test_ds = Dataset(test_reviews_vectorized, test_df['sentiment'].tolist())\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    best_val_acc = 0\n",
    "    for epoch_n in range(epochs):\n",
    "        print(f\"Epoch #{epoch_n + 1}\")\n",
    "        model.train()\n",
    "        for batch in train_dl:\n",
    "            model.zero_grad()\n",
    "\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.float().long()\n",
    "            targets = targets\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validare\n",
    "        model.eval()\n",
    "        all_predictions = torch.tensor([])\n",
    "        all_targets = torch.tensor([])\n",
    "        for batch in val_dl:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.float().long()\n",
    "            targets = targets\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(inputs)\n",
    "\n",
    "            predictions = output.argmax(1)\n",
    "            all_targets = torch.cat([all_targets, targets.detach().cpu()])\n",
    "            all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n",
    "\n",
    "        val_acc = (all_predictions == all_targets).float().mean().numpy()\n",
    "        print(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), \"./data/best_model.pt\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    print(\"Best validation accuracy\", best_val_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "0.8737778\n",
      "Epoch #2\n",
      "0.8513333\n",
      "Epoch #3\n",
      "0.88\n",
      "Epoch #4\n",
      "0.89155555\n",
      "Epoch #5\n",
      "0.89533335\n",
      "Epoch #6\n",
      "0.8946667\n",
      "Epoch #7\n",
      "0.8828889\n",
      "Epoch #8\n",
      "0.8948889\n",
      "Epoch #9\n",
      "0.89622223\n",
      "Epoch #10\n",
      "0.894\n",
      "Epoch #11\n",
      "0.8706667\n",
      "Epoch #12\n",
      "0.888\n",
      "Epoch #13\n",
      "0.8937778\n",
      "Epoch #14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-82-4c98a29cd9a7>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-81-68724341e224>\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(epochs)\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m             \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m         \u001B[0;31m# validare\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/optim/optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m                     \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     26\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     27\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 28\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     29\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mF\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/optim/adam.py\u001B[0m in \u001B[0;36mstep\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    131\u001B[0m                     \u001B[0mstate_steps\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstate\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'step'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 133\u001B[0;31m             F.adam(params_with_grad,\n\u001B[0m\u001B[1;32m    134\u001B[0m                    \u001B[0mgrads\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    135\u001B[0m                    \u001B[0mexp_avgs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/optim/_functional.py\u001B[0m in \u001B[0;36madam\u001B[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001B[0m\n\u001B[1;32m     92\u001B[0m             \u001B[0mdenom\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mmax_exp_avg_sqs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 94\u001B[0;31m             \u001B[0mdenom\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mexp_avg_sq\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mmath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbias_correction2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0meps\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m         \u001B[0mstep_size\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0mbias_correction1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train_model(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    model.eval()\n",
    "    all_predictions = torch.tensor([])\n",
    "    all_targets = torch.tensor([])\n",
    "    for batch in test_dl:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.float().long()\n",
    "        targets = targets\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "\n",
    "        predictions = output.argmax(1)\n",
    "        all_targets = torch.cat([all_targets, targets.detach().cpu()])\n",
    "        all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n",
    "\n",
    "    val_acc = (all_predictions == all_targets).float().mean().numpy()\n",
    "    print(val_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate test data:\n",
      "0.8958\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./data/best_model.pt\"))\n",
    "print(\"Evaluate test data:\")\n",
    "eval_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}