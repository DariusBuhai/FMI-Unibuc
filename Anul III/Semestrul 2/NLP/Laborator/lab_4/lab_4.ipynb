{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c959d05",
   "metadata": {
    "id": "6c959d05"
   },
   "source": [
    "# Lab 4. Introducere PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab18a4",
   "metadata": {
    "id": "99ab18a4"
   },
   "source": [
    "### Ce este PyTorch?\n",
    "\n",
    "PyTorch este o biblioteca Python folosita cu doua scopuri:\n",
    " * reprezinta un inlocuitor al bibliotecii NumPy, care se poate folosi de puterea computationala a componentelor precum GPUs\n",
    " * contine implementarea unui sistem automat de derivare, util pentru antrenarea retelelor neuronale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3545f501",
   "metadata": {
    "id": "3545f501"
   },
   "outputs": [],
   "source": [
    "# ! pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929fa470",
   "metadata": {
    "id": "929fa470"
   },
   "source": [
    "## Tensori\n",
    "\n",
    "Tensorii sunt o structura de date speciala, asemanatoare vectorilor si matricelor. Tensorii sunt folositi in PyTorch pentru a stoca inputul si outputul modelelor, dar si parametrii modelelor.\n",
    "\n",
    "Tensorii se aseamana foarte mult cu NumPy arrays, cu diferenta ca pot fi procesati de GPU sau alte tipuri de hardware specializat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0696ee2",
   "metadata": {
    "id": "c0696ee2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2305fe",
   "metadata": {
    "id": "4d2305fe"
   },
   "source": [
    "### Initializarea tensorilor\n",
    "\n",
    "**Folosind Python lists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c15fb0",
   "metadata": {
    "id": "84c15fb0",
    "outputId": "b2e821ef-5f48-4706-d133-904d20dbcf7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "t_from_list = torch.tensor(data)\n",
    "print(t_from_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2989ee3",
   "metadata": {
    "id": "d2989ee3"
   },
   "source": [
    "**Folosind NumPy arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7d55ce",
   "metadata": {
    "id": "cb7d55ce",
    "outputId": "c72eae92-0633-4738-a3e2-c9b0d41682b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "np_array = np.array(data)\n",
    "t_from_np = torch.from_numpy(np_array)\n",
    "print(t_from_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8d2d7c",
   "metadata": {
    "id": "1c8d2d7c"
   },
   "source": [
    "**Folosind un alt tensor**\n",
    "\n",
    "Noul tensor retine proprietatile (shape + tip de date) ale tensorului original, daca nu este specificat explicit altfel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94017d07",
   "metadata": {
    "id": "94017d07",
    "outputId": "7c050a70-bee7-4647-abb2-09353588ba63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor plin de 1:\n",
      "tensor([[1, 1],\n",
      "        [1, 1]])\n",
      "\n",
      "Tensor random:\n",
      "tensor([[0.2685, 0.9653],\n",
      "        [0.8737, 0.9983]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_ones = torch.ones_like(t_from_list)  # aceleasi proprietati cu ale lui t_from_list\n",
    "print(f\"Tensor plin de 1:\\n{t_ones}\\n\")\n",
    "\n",
    "t_rand = torch.rand_like(t_from_list, dtype=torch.float)  # suprascrie tipul de date\n",
    "print(f\"Tensor random:\\n{t_rand}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc2f9b7",
   "metadata": {
    "id": "9cc2f9b7"
   },
   "source": [
    "**Folosind valori aleatoare sau constante**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205baa85",
   "metadata": {
    "id": "205baa85",
    "outputId": "d82b7bbd-f107-4008-834f-4505caee9ed7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor:\n",
      "tensor([[0.1772, 0.1739, 0.9019],\n",
      "        [0.8926, 0.6885, 0.9407]])\n",
      "\n",
      "Ones tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "Zeros tensor:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3)\n",
    "rand_tensor = torch.rand(shape)\n",
    "ones_tensor = torch.ones(shape)\n",
    "zeros_tensor = torch.zeros(shape)\n",
    "\n",
    "print(f\"Random tensor:\\n{rand_tensor}\\n\")\n",
    "print(f\"Ones tensor:\\n{ones_tensor}\\n\")\n",
    "print(f\"Zeros tensor:\\n{zeros_tensor}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787ee9d",
   "metadata": {
    "id": "2787ee9d"
   },
   "source": [
    "### Proprietatile tensorilor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16961ff1",
   "metadata": {
    "id": "16961ff1",
    "outputId": "e2ae2881-f6b6-4d18-a862-a33abc9bdd0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([3, 4])\n",
      "Tip de date: torch.float32\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print(f\"Tip de date: {tensor.dtype}\")\n",
    "print(f\"Device: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d5ce8",
   "metadata": {
    "id": "519d5ce8"
   },
   "source": [
    "### Operatii cu tensori\n",
    "\n",
    "Puteti gasi [aici](https://pytorch.org/docs/stable/torch.html) o lista exhaustiva cu operatii (transpuneri, indexari, slicing, concatenari, operatii matematice, etc.)\n",
    "\n",
    "Toate aceste operatii pot fi realizate mai rapid cu ajutorul unui GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4acc8ab",
   "metadata": {
    "id": "f4acc8ab",
    "outputId": "8e18618e-a4b1-4a42-8447-f93c5bf7c27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorul este stocat acum pe: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# mutam tensorul pe GPU daca avem unul la dispozitie\n",
    "if torch.cuda.is_available():\n",
    "    tensor = tensor.to(\"cuda\")\n",
    "    print(f\"Tensorul este stocat acum pe: {tensor.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10fe48",
   "metadata": {
    "id": "9b10fe48"
   },
   "source": [
    "**Indexdare si slicing (asemanator NumPy)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a717e83",
   "metadata": {
    "id": "9a717e83",
    "outputId": "e2e3aba7-605e-491f-e80c-4dcc9560857b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.ones(4, 4)\n",
    "tensor[:, 1] = 0\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef6ed5a",
   "metadata": {
    "id": "1ef6ed5a"
   },
   "source": [
    "**Concatenare**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "121cfec3",
   "metadata": {
    "id": "121cfec3",
    "outputId": "fa251f73-8605-4297-e83b-8e133b324653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95c80a8",
   "metadata": {
    "id": "d95c80a8"
   },
   "source": [
    "**Inmultirea tensorilor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a2af9bc",
   "metadata": {
    "id": "9a2af9bc",
    "outputId": "3a58ab44-790a-4176-d27d-c59d6fee1e2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.mul(tensor)\n",
      "tensor([[0, 4],\n",
      "        [4, 0],\n",
      "        [4, 4]])\n",
      "\n",
      "tensor * tensor\n",
      "tensor([[0, 4],\n",
      "        [4, 0],\n",
      "        [4, 4]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inmultire \"element cu element\"\n",
    "tensor = torch.tensor([[0, 2], [2, 0], [2, 2]])\n",
    "print(f\"tensor.mul(tensor)\\n{tensor.mul(tensor)}\\n\")\n",
    "print(f\"tensor * tensor\\n{tensor * tensor}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093d0c92",
   "metadata": {
    "id": "093d0c92",
    "outputId": "c7b7695c-f251-48db-dbb9-1bde4b7c66d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor.matmul(tensor.T)\n",
      "tensor([[4, 0, 4],\n",
      "        [0, 4, 4],\n",
      "        [4, 4, 8]])\n",
      "\n",
      "tensor @ tensor.T\n",
      "tensor([[4, 0, 4],\n",
      "        [0, 4, 4],\n",
      "        [4, 4, 8]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inmultire de matrici\n",
    "# inmultim matricea cu transpusa (tensor.T)\n",
    "print(f\"tensor.matmul(tensor.T)\\n{tensor.matmul(tensor.T)}\\n\")\n",
    "print(f\"tensor @ tensor.T\\n{tensor @ tensor.T}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a24fc07",
   "metadata": {
    "id": "4a24fc07"
   },
   "source": [
    "**Operatii in-place**\n",
    "\n",
    "Operatiile care au `_` ca sufix modifica tensorul pentru care au fost apelate. Exemplu: `x.copy_(y)` sau `x.t_()` il modifica pe `x`.\n",
    "\n",
    "Aceste metode salveaza memorie insa aduc probleme atunci cand vine vorba de calculul automat al derivatelor operatiilor efectuate (de aceea folosirea lor NU este recomandata)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4843ffc4",
   "metadata": {
    "id": "4843ffc4",
    "outputId": "0e03dd7b-8154-45ba-e5e4-bc51c547fd85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 2],\n",
      "        [2, 0],\n",
      "        [2, 2]]) \n",
      "\n",
      "tensor([[5, 7],\n",
      "        [7, 5],\n",
      "        [7, 7]])\n"
     ]
    }
   ],
   "source": [
    "print(tensor, \"\\n\")\n",
    "tensor.add_(5)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bf769b",
   "metadata": {
    "id": "72bf769b"
   },
   "source": [
    "### Legatura cu NumPy\n",
    "\n",
    "Un tensor stocat pe CPU si un NumPy array pot imparti aceeasi memorie in program, astfel modificarile asupra unuia se reflecta asupra celuilalt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e8fd02f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "error",
     "timestamp": 1646764306313,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "9e8fd02f",
    "outputId": "31e536f6-2d06-4cf7-9974-1c86d281fdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.])\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.])\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# tensor -> np array\n",
    "t = torch.ones(5)\n",
    "print(f\"t: {t}\")\n",
    "n = t.numpy()\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "t.add_(1)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51f2dfec",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1646764306312,
     "user": {
      "displayName": "Bogdan Iordache",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh-PZvTGY_WfhB97fLgLmkcd8CIjhKRDgsFQiip=s64",
      "userId": "11937921055418047811"
     },
     "user_tz": -120
    },
    "id": "51f2dfec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "n: [1. 1. 1. 1. 1.]\n",
      "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "n: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# np array -> tensor\n",
    "n = np.ones(5)\n",
    "t = torch.from_numpy(n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")\n",
    "\n",
    "np.add(n, 1, out=n)\n",
    "print(f\"t: {t}\")\n",
    "print(f\"n: {n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0963db",
   "metadata": {
    "id": "ee0963db"
   },
   "source": [
    "## torch.autograd\n",
    "\n",
    "`torch.autograd` este motorul de derivare automata folosit de PyTorch pentru antrenarea retelelor neuronale.\n",
    "\n",
    "### Retele neuronale\n",
    "\n",
    "Retelele neuronale (NN - Neural Networks) sunt o colectie de functii inlantuite care sunt aplicate asupra unor date de intrare. Aceste functii se definesc prin parametrii (weights and biases), care in PyTorch sunt stocate prin intermediul tensorilor.\n",
    "\n",
    "Antrenarea unei retele neuronale se face in doi pasi:\n",
    " - **forward propagation**: reteaua incearca sa prezica output-ul corect pe baza datelor de intrare. Paseaza datele de intrare prin sirul de functii (input-ul functiei urmatoare este output-ul functiei curente din lista)\n",
    " - **backward propagation**: in acest pas parametrii retelei sunt ajustati proportional cu eroare produsa de predictia facuta la pasul forward. Acest lucru este realizat prin a parcurge lista de functii invers, pornind de la output, calculand la fiecare pas derivata erorii in raport cu parametrii functiilor (acestor derivate le mai spunem gradienti), iar parametrii sunt optimizati folosind coborarea pe gradient (gradient descent).\n",
    " \n",
    "### Implementarea in PyTorch\n",
    "\n",
    "Vom incerca un singur pas de antrenare. Incarcam un model preantrenat `resnet18` folosit pentru clasificarea de imagini in $1000$ de clase. Cream un tensor random pentru o singura imagine de $64 \\times 64$ pixeli cu $3$ canale de culoare. Initializam label-ul acestei imagini cu un tensor random de dimensiune $(1, 1000)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4224b94",
   "metadata": {
    "id": "e4224b94"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "# vom considera pentru moment acest model ca un black-box\n",
    "# care primeste ca input tensori de forma (batch_size, 3, 64, 64)\n",
    "# si returneaza pentru fiecare imagine din batch distributia labelurilor pentru acea imagine\n",
    "# forma outputului este deci: (batch_size, 1000)\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e230da27",
   "metadata": {
    "id": "e230da27"
   },
   "outputs": [],
   "source": [
    "# pasul forward\n",
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5836b9",
   "metadata": {
    "id": "db5836b9"
   },
   "source": [
    "Folosim predictia modelului si labelul corect pentru a calcula eroarea (`loss`). Pasul urmator este propagarea inapoi (backpropagation) a acestei erori prin retea. Acest lucru se realizeaza cand apelam metoda `.backword()` pentru tensorul ce contine eroarea. Motorul `autograd` calculeaza si stocheaza gradientii pentru fircare parametru al modelului in atributul `.grad` al acestor parametrii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67f94068",
   "metadata": {
    "id": "67f94068"
   },
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()  # pasul backward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d613272",
   "metadata": {
    "id": "6d613272"
   },
   "source": [
    "Apoi, incarcam un algoritm de optimizare a parametrilor (optimizer), in cazul de fata [SGD](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d) (Stochastic Gradient Descent) cu learning rate de $0.01$ si momentum $0.9$.\n",
    "\n",
    "Mai intai inregistram toti parametrii modelului in optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4577e17f",
   "metadata": {
    "id": "4577e17f"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd69f0",
   "metadata": {
    "id": "dfcd69f0"
   },
   "source": [
    "In cele din urma, apelam `.step()` pentru optimizer pentru a initia algoritmul de coborare pe gradient. Optimizerul ajusteaza astfel fiecare parametru al modelului pe baza gradientilor calculati la pasul backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16cba403",
   "metadata": {
    "id": "16cba403"
   },
   "outputs": [],
   "source": [
    "optim.step()  # gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_o3EqaUROyuZ",
   "metadata": {
    "id": "_o3EqaUROyuZ"
   },
   "source": [
    "[Alti](https://pytorch.org/docs/stable/optim.html) optimizeri utili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfc3ef",
   "metadata": {
    "id": "08dfc3ef"
   },
   "source": [
    "### Derivarea automata in Autograd\n",
    "\n",
    "In exemplele urmatoare vom vedea cum `autograd` colecteaza si calculeaza gradientii. Cream doi tensori cu atributul `requires_grad` setat pe `True`. Acest lucru semnalizeaza motorului `autograd` ca toate operatiile in care sunt implicati acesti tensori trebuie urmarite (tracked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edacb1b4",
   "metadata": {
    "id": "edacb1b4"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2c7bea",
   "metadata": {
    "id": "4a2c7bea"
   },
   "source": [
    "Cream acum un tensor care retine o operatie intre `a` si `b`.\n",
    "$$Q = 3a^3-b^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e8e3da4",
   "metadata": {
    "id": "5e8e3da4"
   },
   "outputs": [],
   "source": [
    "Q = 3 * a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c79c1f",
   "metadata": {
    "id": "32c79c1f"
   },
   "source": [
    "Daca presupunem ca $a$ si $b$ ar fi parametrii unei retele si $Q$ ar fie eroarea, la antrenare ne intereseaza derivata erorii in raport cu fiecare parametru in parte:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial b} = -2b$$\n",
    "\n",
    "Cand apelam `.backward()` pentru `Q`, `autograd` calculeaza acesti gradienti si ii salveza in atributul `.grad` al tensorilor corespunzatori.\n",
    "\n",
    "In acest exemplu trebuie sa pasam explicit un argument `gradient` functiei `Q.backward()` intrucat `Q` este vector. `gradient` este un tensor de aceeasi forma ca si `Q` si reprezinta derivata lui $Q$ in raport cu el insusi:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial Q} = 1$$\n",
    "\n",
    "Alternativ, putem agrega $Q$ intr-o valoare scalara si sa aplicam `backward` fara a mai specifica gradientul initial: `Q.sum().backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7488dae6",
   "metadata": {
    "id": "7488dae6"
   },
   "outputs": [],
   "source": [
    "# Q este vector specificam derivata cu el insusi\n",
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)\n",
    "\n",
    "# agregam Q intr-un scalar\n",
    "# Q.sum().backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8044cb",
   "metadata": {
    "id": "fb8044cb"
   },
   "source": [
    "Gradientii sunt stocati acum in `a.grad` si `b.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b114c917",
   "metadata": {
    "id": "b114c917",
    "outputId": "e062356e-0a97-49d5-ce9b-e080618eea9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9a^2 = tensor([36., 81.], grad_fn=<MulBackward0>)\n",
      "dQ/da = tensor([36., 81.])\n",
      "\n",
      "-2b = tensor([-12.,  -8.], grad_fn=<MulBackward0>)\n",
      "dQ/db = tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"9a^2 = {9 * a ** 2}\")\n",
    "print(f\"dQ/da = {a.grad}\\n\")\n",
    "print(f\"-2b = {-2 * b}\")\n",
    "print(f\"dQ/db = {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d3729",
   "metadata": {
    "id": "142d3729"
   },
   "source": [
    "### Graful computatiilor\n",
    "\n",
    "Conceptual, `autograd` retine tensorii si toate operatiile executate asupra acestora, impreuna cu rezultatele intermediare, sub forma unui graf orientat aciclic (DAG - Directed Acyclic Graph) in care nodurile sunt obiecte de tip [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function). In acest DAG, frunzele sunt tensorii din input, iar radacina/radacinile sunt tensorii output. Parcurgand drumuri in acest graf de la radacini la frunze, gradientii pot fi calculati automat folosind \"chain rule\":\n",
    "\n",
    "$$\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}$$\n",
    "\n",
    "In pasul forward, `autograd`:\n",
    " * calculeaza rezultatul operatiei curente intr-un tensor\n",
    " * mentine functia derivata a acestei operatii in DAG, in atributul `.grad_fn` al tensorului rezultat\n",
    " \n",
    "Pasul backward incepe cand este apelata metoda `.backward()` pentru o radacina din DAG. Atunci, `autograd`:\n",
    " * calculeaza fiecare gradient folosind `.grad_fn`\n",
    " * acumuleaza gradientul in atributul `.grad` al tensorului respectiv\n",
    " * folosind \"chain rule\" propaga gradientii pana la frunze\n",
    " \n",
    "Reprezentarea DAG-ului pentru calculul de mai devreme al lui `Q` este aceasta:\n",
    "\n",
    "![](https://pytorch.org/tutorials/_images/dag_autograd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af78243d",
   "metadata": {
    "id": "af78243d",
    "outputId": "4ddaecc4-ef08-400a-d640-0aca608ad482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SubBackward0 object at 0x7f50f873dbe0>\n",
      "((<MulBackward0 object at 0x7f50f873ddc0>, 0), (<PowBackward0 object at 0x7f50f873d5b0>, 0))\n"
     ]
    }
   ],
   "source": [
    "fn = Q.grad_fn\n",
    "print(fn)\n",
    "print(fn.next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02f5ea4",
   "metadata": {
    "id": "b02f5ea4"
   },
   "source": [
    "### Excluderea din DAG\n",
    "\n",
    "`autograd` urmareste operatiile efectuate pe toti tensorii care au `requires_grad=True`. Pentru tensorii ca nu au nevoie de gradienti, setarea acestui atribut cu `False` ii exclude din DAG.\n",
    "\n",
    "Tensorul output al unei operatii va avea `requires_grad=True` daca macar un tensor input are de asemenea `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21b31e6f",
   "metadata": {
    "id": "21b31e6f",
    "outputId": "c3488402-fa67-44dd-e4f7-439402109051"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.requires_grad: False\n",
      "b.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "print(f\"a.requires_grad: {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"b.requires_grad: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54851244",
   "metadata": {
    "id": "54851244"
   },
   "source": [
    "## Pasii de antrenare a unui model\n",
    "\n",
    "Vom ilustra pasii ce trebuie urmati in cazul in care vrem sa definim si antrenam un model pentru o problema simpla de clasificare.\n",
    "\n",
    "Mai intai vom defini datele noastre, un set foarte mic de date clasificate in 3 clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1cc36df",
   "metadata": {
    "id": "b1cc36df"
   },
   "outputs": [],
   "source": [
    "dummy_data = {\n",
    "    \"vectors\": [\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 2., 3., 4.]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 4., 2., 5.]),\n",
    "        torch.tensor([0., 0., 0., 0., 0., 0., 7., 9., 1.]),\n",
    "        torch.tensor([0., 0., 0., 4., 5., 2., 0., 0., 0.]),\n",
    "        torch.tensor([0., 0., 0., 2., 4., 6., 0., 0., 0.]),\n",
    "        torch.tensor([0., 0., 0., 2., 3., 5., 0., 0., 0.]),\n",
    "        torch.tensor([2., 9., 6., 0., 0., 0., 0., 0., 0.]),\n",
    "        torch.tensor([6., 5., 4., 0., 0., 0., 0., 0., 0.]),\n",
    "        torch.tensor([7., 7., 4., 0., 0., 0., 0., 0., 0.]),\n",
    "    ],\n",
    "    \"labels\": [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff843bd",
   "metadata": {
    "id": "aff843bd"
   },
   "source": [
    "### PyTorch Dataset\n",
    "\n",
    "Vom incarca setul nostru de date intr-un obiect din clasa [Dataset](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f9b54ba",
   "metadata": {
    "id": "1f9b54ba"
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        \n",
    "    # urmatoarele doua metode trebuie implementate\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n",
    "        return (\n",
    "            self.data[\"vectors\"][k],\n",
    "            self.data[\"labels\"][k]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n",
    "        return len(self.data[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e084905b",
   "metadata": {
    "id": "e084905b",
    "outputId": "2f720965-bef6-444d-f95c-d845c1626bdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dataset = MyDataset(dummy_data)\n",
    "len(dummy_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9307cbd2",
   "metadata": {
    "id": "9307cbd2"
   },
   "source": [
    "### PyTorch DataLoader\n",
    "\n",
    "In timpul antrenarii vrem sa parcurgem exemplele din dataset si sa le trecem prin reteaua noastra. In practica, mai multe exemple sunt trecute prin retea inainte de a apela pasul backward. Un astfel de grup se numeste batch.\n",
    "\n",
    "DataLoaderul este un iterator construit plecand de la dataset, care grupeaza mai multe exemple in batch-uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8982b8b3",
   "metadata": {
    "id": "8982b8b3"
   },
   "outputs": [],
   "source": [
    "dummy_dataloader = torch.utils.data.DataLoader(dummy_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b50a094",
   "metadata": {
    "id": "1b50a094",
    "outputId": "7d9e3e1e-39ae-40c0-ab80-67dc22ed94f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0., 0., 0., 0., 0., 0., 7., 9., 1.],\n",
       "         [6., 5., 4., 0., 0., 0., 0., 0., 0.]]),\n",
       " tensor([0, 2])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dummy_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdde618",
   "metadata": {
    "id": "9cdde618"
   },
   "source": [
    "### Definitia modelului\n",
    "\n",
    "Dupa cum am spus mai devreme, un model este o inlantuire de functii (sau straturi/layere). Pentru a defini un model implementam o clasa care mosteneste clasa `torch.nn.Module` si implementam constructorul acesteia in care definim layerele si metoda `forward` in care descriem cum sunt aplicate aceste layere.\n",
    "\n",
    "Vom implementa un model foarte simplu: un singur layer linear (feed-forward) care primeste un vector de dimensiune $9$ (numarul de features ale datelor noastre) si returneaza un output de dimensiune $3$ (numarul de clase din problema noastra).\n",
    "\n",
    "Un layer linear este o operatie de forma $y = A \\cdot x + b$ (unde matricea $A$ reprezinta weight-urile layerului, iar vectorul $b$, biasul)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d94d071",
   "metadata": {
    "id": "6d94d071"
   },
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features=9, out_features=3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)  # aplicam layerul linear\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa02e194",
   "metadata": {
    "id": "fa02e194"
   },
   "source": [
    "### Calculul erorii\n",
    "\n",
    "Pentru un vector de dimensiune $9$ modelul returneaza unul de dimensiune $3$. Cum putem interpreta acest vector pentru a obtine predictiile?\n",
    "\n",
    "Am putea spre exemplu sa determinam pozitia valorii maxime si sa spunem ca aceea este clasa prezisa de model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09ed89a1",
   "metadata": {
    "id": "09ed89a1",
    "outputId": "b6461db8-bfd7-4bda-87b5-11ff580fc881"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxim:\t\t tensor(4.)\n",
      "Poz. maxim:\t tensor(1)\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([3., 4., 1.])\n",
    "print(\"Maxim:\\t\\t\", t.max(0)[0])\n",
    "print(\"Poz. maxim:\\t\", t.max(0)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bcfb2",
   "metadata": {
    "id": "0b9bcfb2"
   },
   "source": [
    "Insa o varianta mai buna ar fi sa transformam valorile tensorului intr-o distributie de probabilitate (cu ce probabilitate atribuie modelul clasa $i$ exemplului?).\n",
    "\n",
    "O functie care calculeaza o astfel de distributie pornind de la un tensor oarecare este functia [*softmax*](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9c76396",
   "metadata": {
    "id": "b9c76396",
    "outputId": "1e512338-dd47-4458-ef91-1cc18ccd57a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2595, 0.7054, 0.0351])\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.functional.softmax(t, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6ac26",
   "metadata": {
    "id": "a3d6ac26"
   },
   "source": [
    "Folosind distributia generata de model si labelurile adevarate putem calcula acum eroarea. O functie de eroare des folosita pentru clasificare este [*cross-entropy*](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "136b0a47",
   "metadata": {
    "id": "136b0a47",
    "outputId": "c92ec6eb-09fd-486e-f81c-0614d930bef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeluri prezise: tensor([1, 0])\n",
      "loss:  tensor(1.2367)\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# presupunem ca aceasta este output-ul modelului pentru un batch de 2 exemple\n",
    "output = torch.tensor([\n",
    "    [3., 7., 4.],\n",
    "    [4., 2., 3.],\n",
    "])\n",
    "\n",
    "# si presupunem ca aceastea ar fi labelurile adevarate\n",
    "true_labels = torch.tensor([1, 1])\n",
    "\n",
    "print(\"Labeluri prezise:\", output.max(1)[1])\n",
    "\n",
    "# functia de loss aplica automat softmax peste output-ul modelului\n",
    "# rezultatul este un scalar corespunzator erorii medii pentru toate exemplele din batch\n",
    "print(\"loss: \", loss_fn(output, true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8088f",
   "metadata": {
    "id": "d8b8088f"
   },
   "source": [
    "### O epoca de antrenare\n",
    "\n",
    "O epoca de antrenare reprezinta parcurgerea datasetului (o data) si actualizarea parametrilor modelului pe baza predictiilor facute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09ca4e1a",
   "metadata": {
    "id": "09ca4e1a"
   },
   "outputs": [],
   "source": [
    "# cuda daca avem la dispozitie GPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# initializam modelul si il mutam pe device-ul corespunzator\n",
    "model = Model().to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# setam modelul in modul de antrenare\n",
    "model.train()\n",
    "\n",
    "# parcurgem batch-urile folosind dataloaderul\n",
    "for (vectors, labels) in dummy_dataloader:\n",
    "    # mutam vectorii si labelurile pe device-ul corespunzator\n",
    "    vectors = vectors.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    \n",
    "    # reseteaza gradientii ramasi de la batch-ul anterior\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # trecem vectorii (batch_size x 9) din batch prin model (forward step)\n",
    "    output = model(vectors)  # shape: batch_size x 3\n",
    "    \n",
    "    # calculam loss-ul\n",
    "    loss = loss_fn(output, labels)\n",
    "    \n",
    "    # backward step\n",
    "    loss.backward()\n",
    "    \n",
    "    # actualizam parametrii modelului folosind optimizerul\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5045644",
   "metadata": {
    "id": "f5045644"
   },
   "source": [
    "Acum putem evalua modelul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a7f6ff81",
   "metadata": {
    "id": "a7f6ff81"
   },
   "outputs": [],
   "source": [
    "# trecem modelul in modul de evaluare\n",
    "model.eval()\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "mean_loss = 0\n",
    "\n",
    "with torch.no_grad():  # dezactivam engine-ul autograd\n",
    "    for (vectors, labels) in dummy_dataloader:\n",
    "        vectors = vectors.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        output = model(vectors)\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        mean_loss += loss.item()\n",
    "        true_labels.extend(labels.tolist())\n",
    "        predicted_labels.extend(output.max(1)[1].tolist())\n",
    "\n",
    "mean_loss /= len(dummy_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fdd5fa88",
   "metadata": {
    "id": "fdd5fa88",
    "outputId": "3c5e0bd9-76c9-4082-895b-3a023bae14b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Mean loss: 0.11842860709875822\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(true_labels, predicted_labels))\n",
    "print(\"Mean loss:\", mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4159d6",
   "metadata": {
    "id": "4f4159d6"
   },
   "source": [
    "#### Important !!!\n",
    "\n",
    "Retelele neuronale pot obtine destul de usor performanta foarte buna pe datele cu care au fost antrenate, dar fara sa generalizeze (pe date noi, obtin performanta foarte slaba). Numim acest caz **overfit**.\n",
    "\n",
    "Pentru a stabili clar performanta modelului in practica se realizeaza o impartire a datelor in 3 categorii:\n",
    " - train: datele pe care este antrenat modelul la fiecare epoca\n",
    " - validare: la finalul fiecarei epoci evaluam performanta modelului pe aceste date (nevazute in timpul antrenarii)\n",
    " - test: odata ce am antrenat cel mai bun model (selectat conform datelor de validare) evaluarea finala este facuta pe aceste date (de asemenea, tot nevazute in timpul antrenarii)\n",
    " \n",
    "Datele de validare sunt folosite pentru a monitoriza performanta (loss, acuratete, etc.) modelului la finalul fiecarei epoci. Putem selecta de exemplu parametrii modelului de la finalul epocii cu cea mai bune performanta. De asemenea, daca observam ca eroare pe datele de validare stagneaza sau creste in timp ce eroarea pe datele de train continua sa scade, putem trage concluzia ca modelul a inceput sa faca **overfit** pe datele de train."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb84b8",
   "metadata": {
    "id": "83bb84b8"
   },
   "source": [
    "### Inlantuirea mai multor layere\n",
    "\n",
    "Putem defini de exemplu mai multe layere feed-forward (lineare) si sa trecem input-ul succesiv prin ele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c551e2f1",
   "metadata": {
    "id": "c551e2f1",
    "outputId": "50701e45-19f5-4965-bb71-279a868ba91a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5])\n",
      "Output shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "t_input = torch.tensor([[1., 2., 3., 4., 5.]])  # intotdeauna avem shape (batch_size, dim)\n",
    "\n",
    "layer_1 = torch.nn.Linear(5, 8)\n",
    "layer_2 = torch.nn.Linear(8, 4)\n",
    "layer_3 = torch.nn.Linear(4, 7)\n",
    "\n",
    "t_output = layer_1(t_input)\n",
    "t_output = layer_2(t_output)\n",
    "t_output = layer_3(t_output)\n",
    "\n",
    "print(\"Input shape:\", t_input.shape)\n",
    "print(\"Output shape:\", t_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11b5728",
   "metadata": {
    "id": "b11b5728"
   },
   "source": [
    "#### Functii de activare\n",
    "\n",
    "Functiile de activare (activation functions / non-linearities) sunt functii aplicate asupra outputului unui layer linear. Motivul pentru care avem nevoie de astfel de functii este faptul ca un layer transforma inputul aplicand o transformare afina ($A \\cdot x + b$). Mai multe transformari afine inlantuite sunt echivalente cu o singura transformare (exista un layer `Linear(5, 7)` care ar face aceeasi transformare de mai sus).\n",
    "\n",
    "Astfel adaugand mai multe layere intermediare nu crestem cu adevarat complexitatea modelului. În cazul de mai sus, fără funcție de activare între straturi, modelul are aceeasi complexitate ca și când ar avea un singur strat Linear, și este foarte probabil să facă [underfitting](https://www.ibm.com/cloud/learn/underfitting). Un model care nu este suficient de complex pentru task și nu reușește să învețe este un model **underfit**.\n",
    "\n",
    "Functiile de activare sunt functii nelineare derivabile in toate punctele (eventual cu exceptia unui numar finit de puncte). Functii de activare uzuale sunt [ReLU](https://pytorch.org/docs/1.11/generated/torch.nn.ReLU.html#torch.nn.ReLU) si [tanh](https://pytorch.org/docs/1.11/generated/torch.nn.Tanh.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "131fba78",
   "metadata": {
    "id": "131fba78",
    "outputId": "cd9f0cad-ee51-41f9-e0b9-18e89771562d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5])\n",
      "Output shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "t_input = torch.tensor([[1., 2., 3., 4., 5.]])  # intotdeauna avem shape (batch_size, dim)\n",
    "\n",
    "layer_1 = torch.nn.Linear(5, 8)\n",
    "layer_2 = torch.nn.Linear(8, 4)\n",
    "layer_3 = torch.nn.Linear(4, 7)\n",
    "\n",
    "t_output = layer_1(t_input)\n",
    "t_output = torch.nn.functional.relu(t_output)\n",
    "t_output = layer_2(t_output)\n",
    "t_output = torch.nn.functional.relu(t_output)\n",
    "t_output = layer_3(t_output)\n",
    "\n",
    "print(\"Input shape:\", t_input.shape)\n",
    "print(\"Output shape:\", t_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f0156",
   "metadata": {
    "id": "483f0156"
   },
   "source": [
    "Putem folosi layerul `Sequential` pentru a combina mai multe layere intr-unul singur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72bc96fb",
   "metadata": {
    "id": "72bc96fb",
    "outputId": "76d78929-b8bb-4add-fbb9-e93f44c3cec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5])\n",
      "Output shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "t_input = torch.tensor([[1., 2., 3., 4., 5.]])  # intotdeauna avem shape (batch_size, dim)\n",
    "\n",
    "seq = torch.nn.Sequential(\n",
    "    torch.nn.Linear(5, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(8, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(4, 7),\n",
    ")\n",
    "\n",
    "t_output = seq(t_input)\n",
    "\n",
    "print(\"Input shape:\", t_input.shape)\n",
    "print(\"Output shape:\", t_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a9ec0",
   "metadata": {
    "id": "a89a9ec0"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout-ul este o metoda de regularizare a retelelor neuronale. Acesta poate ajuta la antrenarea unor retele mai robuste care pot generaliza mai bine. Principiul de functionare este simplu:\n",
    " - la antrenare, fiecare componenta a unui vector trecut prin acest layer este transformata in $0$ cu probabilitate $p$, iar restul componentelor sunt scalate cu $\\frac{1}{1-p}$\n",
    " - la inferenta (evaluare), acest layer nu afecteaza in niciun fel inputul\n",
    " \n",
    "Intrucat acest layer are comportament diferit la antrenare fata de evaluare, este important sa avem grija ca modelul sa fie setat in prealabil in configuratia corecta (`model.train()` sau `model.eval()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b43dac39",
   "metadata": {
    "id": "b43dac39",
    "outputId": "ebb40544-c1cd-4dc7-a7be-665fe535dd99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
      "Output: tensor([ 2.,  0.,  0.,  0.,  0., 12.,  0., 16., 18.])\n",
      "Output: tensor([ 0.,  0.,  6.,  8.,  0., 12.,  0., 16.,  0.])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
    "dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "print(\"Input:\", t)\n",
    "output = dropout(t)\n",
    "print(\"Output:\", output)\n",
    "output = dropout(t)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec29a811",
   "metadata": {
    "id": "ec29a811"
   },
   "source": [
    "### Salvarea unui model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71817a18",
   "metadata": {
    "id": "71817a18",
    "outputId": "b5cb31c3-329c-4a0d-bca5-6977502f59a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pt\")  # salveaza weight-urile modelului\n",
    "model.load_state_dict(torch.load(\"model.pt\"))  # incarca weight-urile din fisier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2a22eb",
   "metadata": {
    "id": "fa2a22eb"
   },
   "source": [
    "# TASK\n",
    "\n",
    "### Deadline: 24 martie ora 23:59.\n",
    "### Formular pentru trimiterea temei: https://forms.gle/LKCMJmyhS8Z8m7Rc9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392c7aa",
   "metadata": {
    "id": "9392c7aa"
   },
   "source": [
    "1. Porniti de la un dataset artificial generat cu ajutorul metodei [`make_classification`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) din sklearn (minim 10000 de exemple, cu cel putin 100 de feature-uri relevante, grupate in cel putin 3 clase) sau puteti folosi un toy-dataset de [aici](https://scikit-learn.org/stable/datasets/toy_dataset.html).\n",
    "\n",
    "2. Definiti un model cu cel putin 3 layere lineare, dintre care ultimul are dimensiunea outputului egala cu numarul de clase. Aplicati dupa fiecare layer linear (cu exceptia ultimului) o functie de activare aleasa de voi.\n",
    "\n",
    "3. Impartiti datasetul in 80% train, 10% validare si 10% test. Creati cele trei dataloadere corespunzatoare fiecarui split.\n",
    "\n",
    "4. Definiti functia de loss (cross-entropy) si un optimizer (SGD, Adam, etc.).\n",
    "\n",
    "5. Antrenati modelul pentru mai multe epoci pe datele de train. La finalul fiecarei epoci evaluati performanta modelului pe datele de validare. Monitorizati la fiecare epoca eroarea medie si acuratetea pentru predictiile facute pe datele de train si separat pe datele de validare, pentru ca in cazul in care observati situatia de *overfit* sa puteti opri antrenarea.\n",
    "\n",
    "6. Salvati modelul cu cea mai buna eroare de validare, calculata la finalul epocii respective.\n",
    "\n",
    "7. Plotati pe acelasi grafic evolutia erorii de train si a erorii de validare la finalul fiecarei epoci. Plotati in alt grafic evolutia acuratetii pe datele de train si pe cele de validare.\n",
    "\n",
    "8. Evaluati modelul (eroare, acuratete, macro-f1, etc.) pe datele de test.\n",
    "\n",
    "9. Incercati sa adaugati dupa fiecare layer din model (cu exceptia ultimului) dropout cu un $p$ ales de voi. Analizati daca performanta unui model antrenat astfel este mai buna."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "4f4159d6"
   ],
   "name": "lab_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lab-nlp-an3",
   "language": "python",
   "name": "lab-nlp-an3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
