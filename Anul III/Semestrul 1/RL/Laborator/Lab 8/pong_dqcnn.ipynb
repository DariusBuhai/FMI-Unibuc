{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/vq/vdb45zcs75q74lqkhq_rzzgr0000gn/T/ipykernel_89857/3442724866.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mcv2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import tensorflow.keras as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Wrapper Class for the Gym Pong Environment\n",
    "class Pong:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"PongNoFrameskip-v4\")\n",
    "        self.ACTION_SPACE = [0, 2, 3]\n",
    "        self.env.reset()\n",
    "        self.SKIP_FRAMES = 8\n",
    "        self.NEUTRAL_ACTION = 0\n",
    "        self.FIRE_ACTION = 1\n",
    "        self.framecount = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        o, _, _, _ = self.env.step(self.FIRE_ACTION)\n",
    "        sum = 0\n",
    "        while sum < 70:\n",
    "            o, _, _, _ = self.env.step(self.NEUTRAL_ACTION)\n",
    "            o = self.prep_o(o)\n",
    "            sum = np.sum(o)\n",
    "        self.framecount = 0\n",
    "        return o\n",
    "\n",
    "    def neutral_step(self):\n",
    "        return self.step(self.NEUTRAL_ACTION)\n",
    "\n",
    "    def step(self, action):\n",
    "        rf = 0\n",
    "        self.framecount += 1\n",
    "        done = False\n",
    "        for _ in range(self.SKIP_FRAMES):\n",
    "            o, r, d = self.step_raw(self.ACTION_SPACE[action])\n",
    "            if not r == 0:\n",
    "                rf = r\n",
    "                done = True\n",
    "        return self.prep_o(o), rf, done\n",
    "\n",
    "    def step_raw(self, action):\n",
    "        o, r, d, _ = self.env.step(action)\n",
    "        return o, r, d\n",
    "\n",
    "    def o_imshow(self, o):\n",
    "        o = o * 255.0\n",
    "        o = o.astype(np.uint8)\n",
    "        cv2.imshow('o', o)\n",
    "        cv2.waitKey(20)\n",
    "\n",
    "    def prep_o(self, o):\n",
    "        o = o[35:195, :]\n",
    "        o = cv2.cvtColor(o, cv2.COLOR_RGB2GRAY)\n",
    "        ret, o = cv2.threshold(o, 100, 255, cv2.THRESH_BINARY)\n",
    "        o = cv2.resize(o, (80, 80))\n",
    "        o = o.astype(np.float64).reshape((80, 80, 1))\n",
    "        o = o / 255.0\n",
    "        return o\n",
    "\n",
    "    def render(self):\n",
    "        self.env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#RL Agent Class\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.N_FRAMES = 4\n",
    "        self.framebuffer = deque(maxlen=self.N_FRAMES)\n",
    "        self.pong = Pong()\n",
    "        self.pong.reset()\n",
    "        self.EXPERIENCE_SIZE = 8192\n",
    "        self.experience = deque(maxlen=self.EXPERIENCE_SIZE)\n",
    "        self.stepcount = 0\n",
    "        self.LEARNING_RATE = 0.0001\n",
    "        self.make_models()\n",
    "        self.DISCOUNT = 0.95\n",
    "        self.EPSILON_MAX = 1.0\n",
    "        self.epsilon = self.EPSILON_MAX\n",
    "        self.EPSILON_MIN = 0.02\n",
    "        self.EPSILON_DECAY_STEPS = 10000\n",
    "        self.BATCH_SIZE = 32\n",
    "        self.fit_epoch_count = 1\n",
    "        self.UPDATE_TARGET_EVERY = 1000\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.stepcount += 1\n",
    "        self.epsilon = max(self.EPSILON_MAX - (self.stepcount * (self.EPSILON_MAX / self.EPSILON_DECAY_STEPS)),\n",
    "                           self.EPSILON_MIN)\n",
    "\n",
    "    def play(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            retval = random.choice(range(len(self.pong.ACTION_SPACE)))\n",
    "        else:\n",
    "            retval = np.argmax(self.running_model.predict(np.asarray([state]))[0])\n",
    "        self.update_epsilon()\n",
    "        return retval\n",
    "\n",
    "    def play_random_game(self):\n",
    "        o = self.pong.reset()\n",
    "        for _ in range(self.N_FRAMES):\n",
    "            self.framebuffer.append(o)\n",
    "            o, _, _ = self.pong.neutral_step()\n",
    "        done = False\n",
    "        while not done:\n",
    "            old_state = self.make_state()\n",
    "            action = random.choice(range(len(self.pong.ACTION_SPACE)))\n",
    "            o_new, r, done = self.pong.step(action)\n",
    "            self.framebuffer.append(o_new)\n",
    "            new_state = self.make_state()\n",
    "            self.experience.append((old_state, action, r, new_state, done))\n",
    "\n",
    "    def make_state(self):\n",
    "        return np.concatenate(self.framebuffer, axis=-1)\n",
    "\n",
    "    def validate_experience(self):\n",
    "        old_state, action, reward, new_state, done = random.choice(self.experience)\n",
    "        fig, axs = plt.subplots(2, 4)\n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "                state = old_state\n",
    "            else:\n",
    "                state = new_state\n",
    "            for j in range(self.N_FRAMES):\n",
    "                axs[i][j].imshow(state[:, :, j])\n",
    "        plt.show()\n",
    "\n",
    "    def play_and_learn(self, numgames):\n",
    "        game_lengths = []\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        rewards = []\n",
    "        fig, axs = plt.subplots(2, 2)\n",
    "        axs[0][0].set_title(\"Game Length\")\n",
    "        axs[0][1].set_title(\"Epsilon\")\n",
    "        axs[1][0].set_title(\"Ultimate Reward\")\n",
    "        axs[1][1].set_title(\"Mean DQN Loss\")\n",
    "        plt.subplots_adjust(hspace=0.8, wspace=0.3)\n",
    "        for ep in tqdm(range(numgames)):                \n",
    "            game_losses = []\n",
    "            o = self.pong.reset()\n",
    "            for _ in range(self.N_FRAMES):\n",
    "                self.framebuffer.append(o)\n",
    "                o, _, _ = self.pong.neutral_step()\n",
    "            done = False\n",
    "            while not done:\n",
    "                self.pong.render()\n",
    "                old_state = self.make_state()\n",
    "                action = self.play(old_state)\n",
    "                o_new, r, done = self.pong.step(action)\n",
    "                self.framebuffer.append(o_new)\n",
    "                new_state = self.make_state()\n",
    "                self.experience.append((old_state, action, r, new_state, done))\n",
    "                ls = self.experience_replay()[0]\n",
    "                game_losses.append(ls)\n",
    "            rewards.append(r)\n",
    "            losses.append(sum(game_losses) / len(game_losses))\n",
    "            game_lengths.append(self.pong.framecount)\n",
    "            epsilons.append(self.epsilon)\n",
    "            axs[0][0].plot(game_lengths)\n",
    "            axs[0][1].plot(epsilons)\n",
    "            axs[1][0].plot(rewards)\n",
    "            axs[1][1].plot(losses)\n",
    "            plt.show()\n",
    "            plt.pause(0.01)\n",
    "        return game_lengths\n",
    "\n",
    "    def process_batch(self, batch):\n",
    "        old_states = [x for x, _, _, _, _ in batch]\n",
    "        new_states = [x for _, _, _, x, _ in batch]\n",
    "        old_rewards = self.running_model.predict(np.asarray(old_states))\n",
    "        new_rewards = self.target_model.predict(np.asarray(new_states))\n",
    "        for i, (old_state, action, reward, new_state, done) in enumerate(batch):\n",
    "            add_q = 0\n",
    "            if not done:\n",
    "                add_q = self.DISCOUNT * np.amax(new_rewards[i])\n",
    "            q_value = reward + add_q\n",
    "            old_rewards[i][action] = q_value\n",
    "        return np.asarray(old_states), old_rewards\n",
    "\n",
    "    def experience_replay(self):\n",
    "        batch = random.choices(self.experience, k=32)\n",
    "        x, y = self.process_batch(batch)\n",
    "        hist = self.running_model.fit(x, y, verbose=False)\n",
    "        self.fit_epoch_count += 1\n",
    "        if self.stepcount % self.UPDATE_TARGET_EVERY == 0:\n",
    "            self.target_model.set_weights(self.running_model.get_weights().copy())\n",
    "        return hist.history['loss']\n",
    "\n",
    "    def make_models(self):\n",
    "        self.running_model = tf.Sequential()\n",
    "        self.target_model = tf.Sequential()\n",
    "        self.add_layers_to(self.running_model)\n",
    "        self.add_layers_to(self.target_model)\n",
    "        self.running_model.compile(optimizer=Adam(learning_rate=self.LEARNING_RATE), loss='mse')\n",
    "        self.running_model.summary()\n",
    "\n",
    "    def add_layers_to(self, model: tf.Sequential):\n",
    "        layerlist = [\n",
    "            Conv2D(32, 8, 4, use_bias=False, activation='relu', input_shape=(80, 80, self.N_FRAMES)),\n",
    "            Conv2D(16, 4, 2, use_bias=False, activation='relu'),\n",
    "            Conv2D(16, 3, 1, use_bias=False, activation='relu'),\n",
    "            Flatten(),\n",
    "            Dense(512, use_bias=False, activation='relu'),\n",
    "            Dense(len(self.pong.ACTION_SPACE), use_bias=False, activation='linear')\n",
    "        ]\n",
    "        [model.add(x) for x in layerlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Begin Training\n",
    "a = Agent()\n",
    "a.play_and_learn(10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}