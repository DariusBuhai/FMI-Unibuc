{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TASK\n",
    "## Deadline: 31 martie ora 23:59.\n",
    "\n",
    "Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n",
    "\n",
    "Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n",
    "1. Impartiti setul de date in 80% train, 10% validare si 10% test\n",
    "2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n",
    "3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n",
    "4. Implementati urmatoarea arhitectura:\n",
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n",
    "5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n",
    "6. Evaluati cel mai bun model obtinut pe datele de test.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7f6973ed9190>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from sys import platform, path\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    path.append('/home/dariusbuhai/python/lib/python3.9/site-packages')\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "train_df, test_df = train_test_split(data, test_size=0.1, random_state=1)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/dariusbuhai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def transform_to_tokens(data):\n",
    "    reviews = []\n",
    "    for review in data:\n",
    "        review_tokenized = word_tokenize(review.lower())\n",
    "        reviews.append(review_tokenized)\n",
    "    return reviews\n",
    "\n",
    "train_reviews = transform_to_tokens(train_df.review)\n",
    "test_reviews = transform_to_tokens(test_df.review)\n",
    "val_reviews = transform_to_tokens(val_df.review)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def no_punctuation_or_stopwords(data):\n",
    "    all_words = []\n",
    "    for word in data:\n",
    "        if word not in nlp.Defaults.stop_words and word != ' ' and word not in string.punctuation:\n",
    "            all_words.append(word)\n",
    "    return all_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "def word_freq(data, min_aparitions):\n",
    "\n",
    "    all_words = [words.lower() for sentences in data for words in sentences]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n",
    "    final_vocab = [k for k,v in sorted_vocab if v > min_aparitions]\n",
    "\n",
    "    return final_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def create_vocab(reviews):\n",
    "    vocab = word_freq(reviews, min_aparitions = 18)\n",
    "    vocab = no_punctuation_or_stopwords(vocab)\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "vocab = create_vocab(train_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18409\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sentences(data, char_indices, one_hot = False):\n",
    "    vectorized = []\n",
    "    for sentences in data:\n",
    "\n",
    "        # transformam fiecare review in reprezentarea lui sub forma de indici ale caracterelor continute\n",
    "        sentences_of_indices = [char_indices[w] if w in char_indices.keys() else char_indices['UNK'] for w in sentences]\n",
    "\n",
    "        # pentru fiecare indice putem face reprezentarea one-hot corespunzatoare\n",
    "        # sau putem sa nu facem asta si sa adaugam un embedding layer in model care face aceastÄƒ transformare\n",
    "        if one_hot:\n",
    "            sentences_of_indices = np.eye(len(char_indices))[sentences_of_indices]\n",
    "\n",
    "        vectorized.append(sentences_of_indices)\n",
    "\n",
    "    return vectorized\n",
    "\n",
    "def pad(samples, max_length):\n",
    "\n",
    "    return torch.tensor([\n",
    "        sample[:max_length] + [1] * max(0, max_length - len(sample))\n",
    "        for sample in samples\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def create_vectorize(vocab, reviews):\n",
    "    word_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n",
    "    indices_word = dict((i + 2, c) for i, c in enumerate(vocab))\n",
    "    indices_word[0] = 'UNK'\n",
    "    word_indices['UNK'] = 0\n",
    "    indices_word[1] = 'PAD'\n",
    "    word_indices['PAD'] = 1\n",
    "\n",
    "    reviews_vectorized = vectorize_sentences(reviews, word_indices)\n",
    "    reviews_vectorized = pad(reviews_vectorized, max_length = 800)\n",
    "    return reviews_vectorized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "train_reviews_vectorized = create_vectorize(vocab, train_reviews)\n",
    "test_reviews_vectorized = create_vectorize(vocab, test_reviews)\n",
    "val_reviews_vectorized = create_vectorize(vocab, val_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40500, 800])\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews_vectorized.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n",
    "        return self.samples[k], self.labels[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n",
    "        return len(self.samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(train_reviews_vectorized.shape[0], 100, padding_idx=1)\n",
    "        conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        global_average = torch.nn.AvgPool1d(kernel_size=100, stride=128)\n",
    "        self.convolutions = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.4),\n",
    "            conv1,\n",
    "            conv2,\n",
    "            conv3,\n",
    "            global_average\n",
    "        )\n",
    "\n",
    "        # Flattening layer\n",
    "        flatten = torch.nn.Flatten()\n",
    "\n",
    "        # Linear layer cu 128 input features È™i 2 outputs fÄƒrÄƒ funcÈ›ie de activare\n",
    "        linear = torch.nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(flatten, linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        output = self.convolutions(embeddings)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Adam optimizer cu lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Cross Entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_ds = Dataset(train_reviews_vectorized, train_df['sentiment'].tolist())\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "val_ds = Dataset(val_reviews_vectorized, val_df['sentiment'].tolist())\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "test_ds = Dataset(test_reviews_vectorized, test_df['sentiment'].tolist())\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    best_val_acc = 0\n",
    "    for epoch_n in range(epochs):\n",
    "        print(f\"Epoch #{epoch_n + 1}\")\n",
    "        model.train()\n",
    "        for batch in train_dl:\n",
    "            model.zero_grad()\n",
    "\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.float().long()\n",
    "            targets = targets\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validare\n",
    "        model.eval()\n",
    "        all_predictions = torch.tensor([])\n",
    "        all_targets = torch.tensor([])\n",
    "        for batch in val_dl:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.float().long()\n",
    "            targets = targets\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(inputs)\n",
    "\n",
    "            predictions = output.argmax(1)\n",
    "            all_targets = torch.cat([all_targets, targets.detach().cpu()])\n",
    "            all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n",
    "\n",
    "        val_acc = (all_predictions == all_targets).float().mean().numpy()\n",
    "        print(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), \"./data/best_model.pt\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    print(\"Best validation accuracy\", best_val_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "0.86377776\n",
      "Epoch #2\n"
     ]
    }
   ],
   "source": [
    "train_model(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import metrics\n",
    "\n",
    "\n",
    "def eval_model(dataloader):\n",
    "    model.eval()\n",
    "    true_labels, predicted_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for (vectors, labels) in dataloader:\n",
    "            output = model(vectors)\n",
    "            true_labels.extend(labels.tolist())\n",
    "            predicted_labels.extend(output.max(1)[1].tolist())\n",
    "    accuracy = metrics.accuracy_score(true_labels, predicted_labels)\n",
    "    print(\"Evaluated Accuracy:\", accuracy)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./data/best_model.pt\"))\n",
    "print(\"Evaluate test data:\")\n",
    "eval_model(test_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}