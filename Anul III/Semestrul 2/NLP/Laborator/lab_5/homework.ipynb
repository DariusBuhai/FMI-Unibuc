{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TASK\n",
    "## Deadline: 31 martie ora 23:59.\n",
    "\n",
    "Formular pentru trimiterea temei: https://forms.gle/Bznaciv2MTy4kVL47\n",
    "\n",
    "Folosind intreg datasetul de mai sus (IMDb reviews) implementati urmatoarele cerinte:\n",
    "1. Impartiti setul de date in 80% train, 10% validare si 10% test\n",
    "2. Tokenizati textele si determinati vocabularul (in acest task vom lucra cu reprezentari la nivel de cuvant, NU la nivel de caracter); intrucat vocabularul poate fi foarte mare, incercati sa aplicati una dintre tehnicile mentionate in laborator (10K-20K de cuvinte ar fi o dimensiunea rezonabila a vocabularului)\n",
    "3. Transformati textele in vectori de aceeasi dimensiune folosind indexul vocabularului (alegeti o dimensiune maxima de circa 500-1000 de tokens)\n",
    "4. Implementati urmatoarea arhitectura:\n",
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)\n",
    "5. Antrenati arhitectura folosind cross-entropy ca functie de loss si un optimizer la alegere. La finalul fiecarei epoci evaluati modelul pe datele de validare si salvati weighturile celui mai bun model astfel determinat\n",
    "6. Evaluati cel mai bun model obtinut pe datele de test.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "('IMDB_Dataset.csv', <http.client.HTTPMessage at 0x7fef774f4b20>)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from sys import platform, path\n",
    "if platform == \"linux\" or platform == \"linux2\":\n",
    "    path.append('/home/dariusbuhai/python/lib/python3.9/site-packages')\n",
    "from urllib.request import urlretrieve\n",
    "urlretrieve('https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv', 'IMDB_Dataset.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('IMDB_Dataset.csv')\n",
    "train_df, test_df = train_test_split(data, test_size=0.1, random_state=1)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/dariusbuhai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def transform_to_tokens(data):\n",
    "    reviews = []\n",
    "    for review in data:\n",
    "        review_tokenized = word_tokenize(review.lower())\n",
    "        reviews.append(review_tokenized)\n",
    "    return reviews\n",
    "\n",
    "train_reviews = transform_to_tokens(train_df.review)\n",
    "test_reviews = transform_to_tokens(test_df.review)\n",
    "val_reviews = transform_to_tokens(val_df.review)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def no_punctuation_or_stopwords(data):\n",
    "    all_words = []\n",
    "    for word in data:\n",
    "        if word not in nlp.Defaults.stop_words and word != ' ' and word not in string.punctuation:\n",
    "            all_words.append(word)\n",
    "    return all_words\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "def word_freq(data, min_aparitions):\n",
    "\n",
    "    all_words = [words.lower() for sentences in data for words in sentences]\n",
    "    sorted_vocab = sorted(dict(Counter(all_words)).items(), key=operator.itemgetter(1))\n",
    "    final_vocab = [k for k,v in sorted_vocab if v > min_aparitions]\n",
    "\n",
    "    return final_vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def create_vocab(reviews):\n",
    "    vocab = word_freq(reviews, min_aparitions = 18)\n",
    "    vocab = no_punctuation_or_stopwords(vocab)\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "vocab_train = create_vocab(train_reviews)\n",
    "vocab_test = create_vocab(test_reviews)\n",
    "vocab_val = create_vocab(val_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18409\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_sentences(data, char_indices, one_hot = False):\n",
    "    vectorized = []\n",
    "    for sentences in data:\n",
    "\n",
    "        # transformam fiecare review in reprezentarea lui sub forma de indici ale caracterelor continute\n",
    "        sentences_of_indices = [char_indices[w] if w in char_indices.keys() else char_indices['UNK'] for w in sentences]\n",
    "\n",
    "        # pentru fiecare indice putem face reprezentarea one-hot corespunzatoare\n",
    "        # sau putem sa nu facem asta si sa adaugam un embedding layer in model care face această transformare\n",
    "        if one_hot:\n",
    "            sentences_of_indices = np.eye(len(char_indices))[sentences_of_indices]\n",
    "\n",
    "        vectorized.append(sentences_of_indices)\n",
    "\n",
    "    return vectorized\n",
    "\n",
    "def pad(samples, max_length):\n",
    "\n",
    "    return torch.tensor([\n",
    "        sample[:max_length] + [1] * max(0, max_length - len(sample))\n",
    "        for sample in samples\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "def create_vectorize(vocab, reviews):\n",
    "    word_indices = dict((c, i + 2) for i, c in enumerate(vocab))\n",
    "    indices_word = dict((i + 2, c) for i, c in enumerate(vocab))\n",
    "    indices_word[0] = 'UNK'\n",
    "    word_indices['UNK'] = 0\n",
    "    indices_word[1] = 'PAD'\n",
    "    word_indices['PAD'] = 1\n",
    "\n",
    "    reviews_vectorized = vectorize_sentences(reviews, word_indices)\n",
    "    reviews_vectorized = pad(reviews_vectorized, max_length = 750)\n",
    "    return reviews_vectorized"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "train_reviews_vectorized = create_vectorize(vocab_train, train_reviews)\n",
    "test_reviews_vectorized = create_vectorize(vocab_train, test_reviews)\n",
    "val_reviews_vectorized = create_vectorize(vocab_train, val_reviews)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40500, 750])\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews_vectorized.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        \"\"\"Returneaza al k-lea exemplu din dataset\"\"\"\n",
    "        return self.samples[k], self.labels[k]\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returneaza dimensiunea datasetului\"\"\"\n",
    "        return len(self.samples)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    * un Embedding layer pentru vocabularul determinat, ce contine vectori de dimensiune 100\n",
    "    * un layer dropout cu probabilitate 0.4\n",
    "    * un layer convolutional 1D cu 100 canale de input si 128 de canale de output, dimensiunea kernelului de 3 si padding 1; asupra rezultatului aplicati un layer de [BatchNormalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * un layer convolutional 1D cu 128 canale de input si 128 de canale de output, dimensiunea kernelului de 5 si padding 2; asupra rezultatului aplicati un layer de BatchNormalization cu 128 features; aplicati apoi functia de activare ReLU, iar in cele din urma un strat de max-pooling 1D cu kernel size 2.\n",
    "    * asupra rezultatului ultimului layer, aplicati average-pooling 1D obtinand pentru fiecare canal media tuturor valorilor din vectorul sau corespunzator\n",
    "    * un layer feed-forward (linear) cu dimensiunea inputului 128, si 2 noduri pentru output (pentru clasificare in 0/1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(train_reviews_vectorized.shape[0], 100, padding_idx=1)\n",
    "        conv1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, padding=1),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        conv2_3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, padding=2),\n",
    "            torch.nn.BatchNorm1d(num_features=128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool1d(kernel_size=2),\n",
    "        )\n",
    "        global_average = torch.nn.AvgPool1d(kernel_size=250, stride=250)\n",
    "        self.convolutions = torch.nn.Sequential(\n",
    "            torch.nn.Dropout(p=0.4),\n",
    "            conv1,\n",
    "            conv2_3,\n",
    "            conv2_3,\n",
    "            global_average\n",
    "        )\n",
    "\n",
    "        # Flattening layer\n",
    "        flatten = torch.nn.Flatten()\n",
    "\n",
    "        # Linear layer cu 128 input features și 2 outputs fără funcție de activare\n",
    "        linear = torch.nn.Linear(in_features=128, out_features=2)\n",
    "\n",
    "        self.classifier = torch.nn.Sequential(flatten, linear)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embeddings = self.embedding(input)\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        output = self.convolutions(embeddings)\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Adam optimizer cu lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Cross Entropy loss\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_ds = Dataset(train_reviews_vectorized, train_df['sentiment'].tolist())\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "val_ds = Dataset(val_reviews_vectorized, val_df['sentiment'].tolist())\n",
    "val_dl = DataLoader(val_ds, batch_size=64, shuffle=True)\n",
    "\n",
    "test_ds = Dataset(test_reviews_vectorized, test_df['sentiment'].tolist())\n",
    "test_dl = DataLoader(test_ds, batch_size=64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "def train_model(epochs):\n",
    "    best_val_acc = 0\n",
    "    for epoch_n in range(epochs):\n",
    "        print(f\"Epoch #{epoch_n + 1}\")\n",
    "        model.train()\n",
    "        for batch in train_dl:\n",
    "            model.zero_grad()\n",
    "\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.float().long()\n",
    "            targets = targets\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validare\n",
    "        model.eval()\n",
    "        all_predictions = torch.tensor([])\n",
    "        all_targets = torch.tensor([])\n",
    "        for batch in val_dl:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.float()\n",
    "            targets = targets\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(inputs)\n",
    "\n",
    "            predictions = output.argmax(1)\n",
    "            all_targets = torch.cat([all_targets, targets.detach().cpu()])\n",
    "            all_predictions = torch.cat([all_predictions, predictions.detach().cpu()])\n",
    "\n",
    "        val_acc = (all_predictions == all_targets).float().mean().numpy()\n",
    "        print(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            torch.save(model.state_dict(), \"./data/best_model\")\n",
    "            best_val_acc = val_acc\n",
    "\n",
    "    print(\"Best validation accuracy\", best_val_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1\n",
      "tensor([[[-1.0773, -1.0773, -1.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2093, -0.2093, -0.2093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3362, -0.3362, -0.3362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0518, -0.0518, -0.0518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1336, -1.1336, -1.1336,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9606,  0.9606,  0.9606,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.0773, -1.0773,  0.1988,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2093, -0.2093,  1.3251,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3362, -0.3362,  0.0129,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0518, -0.0518,  0.6140,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1336, -1.1336,  0.3256,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9606,  0.9606, -2.1384,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.0773,  1.0251, -1.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2093, -0.1888, -0.2093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3362, -0.8171, -0.3362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0518, -1.1809, -0.0518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1336, -0.6528, -1.1336,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9606,  1.1570,  0.9606,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.0773,  0.3064, -1.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2093,  1.8296, -0.2093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3362,  0.4541, -0.3362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0518, -1.6099, -0.0518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1336,  0.5415, -1.1336,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9606, -1.6626,  0.9606,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.0773, -1.0773, -1.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.2093, -0.2093, -0.2093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3362, -0.3362, -0.3362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.0518, -0.0518, -0.0518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-1.1336, -1.1336, -1.1336,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.9606,  0.9606,  0.9606,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.6282, -1.0773, -1.0773,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.7832, -0.2093, -0.2093,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.6141, -0.3362, -0.3362,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [-0.5457, -0.0518, -0.0518,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 1.5332, -1.1336, -1.1336,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [-0.3946,  0.9606,  0.9606,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<PermuteBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (128x1x93). Calculated output size: (128x1x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-109-c2db0cad5c9f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtrain_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-75-3d7c1bad1e40>\u001B[0m in \u001B[0;36mtrain_model\u001B[0;34m(epochs)\u001B[0m\n\u001B[1;32m     11\u001B[0m             \u001B[0mtargets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtargets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m             \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m             \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-107-0c7fa6817318>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0membeddings\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0membeddings\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpermute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvolutions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0membeddings\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m         \u001B[0moutput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclassifier\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    139\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 141\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[1;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[0;32m-> 1102\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1103\u001B[0m         \u001B[0;31m# Do not call functions when jit is used\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/python/lib/python3.9/site-packages/torch/nn/modules/pooling.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    534\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    535\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 536\u001B[0;31m         return F.avg_pool1d(\n\u001B[0m\u001B[1;32m    537\u001B[0m             \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mceil_mode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    538\u001B[0m             self.count_include_pad)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Given input size: (128x1x93). Calculated output size: (128x1x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "train_model(100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}